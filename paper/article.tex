\documentclass[11pt,a4paper]{article}

% Core packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}

% Math and symbols
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}

% Tables and figures
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{graphicx}

% Layout and formatting
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{parskip}

% Code listings
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  xleftmargin=0.5em,
  xrightmargin=0.5em
}

% Hyperlinks (load last)
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}

% Bibliography
\usepackage{natbib}
\bibliographystyle{plainnat}

\title{Semantic Compression of LLM Instructions via Symbolic Metalanguages}
\author{Ernst van Gassen\\
  \textit{Independent Researcher}\\
  \texttt{evg.gassen@gmail.com}
}
\date{}

\begin{document}

\maketitle

\begin{abstract}
We introduce MetaGlyph, a symbolic language for compressing prompts by encoding instructions as mathematical symbols rather than prose. Unlike systems requiring explicit decoding rules, MetaGlyph uses symbols like $\in$ (membership) and $\Rightarrow$ (implication) that models already understand from their training data. We test whether these symbols work as ``instruction shortcuts'' that models can interpret without additional teaching.

Across four task types and five models (3B--1T parameters), MetaGlyph achieves 62--81\% fewer tokens while preserving meaning, though reliability varies by symbol and scale. The key finding is scale-dependent operator fidelity: while mid-sized models (7B--12B) show near-zero fidelity for most operators, the frontier-scale Kimi K2 (1T parameters) achieves 98.1\% fidelity for implication ($\Rightarrow$) and 36\% for membership ($\in$). These results suggest a U-shaped relationship between model scale and symbolic comprehension, where sufficient scale can overcome instruction-tuning biases to access pretrained symbolic semantics.
\end{abstract}

\medskip
\noindent\textbf{Keywords:} prompt compression, symbolic metalanguages, instruction semantics, large language models, prompt engineering

\section{Introduction}

Large language models are controlled through natural-language prompts. These prose instructions specify tasks and constraints, and the approach works well enough to be widely adopted. But it has two weaknesses. First, prompts are verbose: expressing precise constraints takes many words. Second, prompt behavior is fragile: small wording changes can produce very different outputs.

These problems have real costs. More tokens mean higher inference costs and latency. Ambiguous phrasing leads to inconsistent results. Yet models are trained on vast amounts of mathematical and logical notation, where compact symbols carry precise meaning. This raises a natural question: can we communicate instructions more directly using those symbols?

Most prompt compression research focuses on shortening the \emph{content} being processed: summarizing documents, pruning passages, dropping ``unimportant'' words. But in many prompts, the \emph{instructions themselves} are a substantial fraction of the total length, especially when specifying scope, exclusions, and output format.

Natural language is inherently imprecise for specifying constraints. Words like ``only,'' ``except,'' and ``roughly'' act like logical operators, but their scope is often unclear when multiple constraints interact. If we could express instructions more compactly using symbols, we might gain both efficiency and reliability.

The key question is: can instruction language be compressed while preserving meaning, \emph{without} teaching the model a new symbolic system?

Existing symbolic prompt languages like SynthLang \citep{synthlang2024} show that models can follow symbolic commands, but only when given explicit decoding rules in a system prompt. This conflates two factors: the symbols themselves, and the instructions teaching the model what they mean.

Other research shows models can learn arbitrary symbol-to-behavior mappings through training \citep{wei2023symbol}. Prompt compression systems like LLMLingua \citep{jiang2023llmlingua,jiang2024llmlingua2} shorten content, not instructions. Adversarial research demonstrates that symbolic encodings can bypass safety mechanisms \citep{bethany2024jailbreaking}, confirming that compact symbols affect model behavior.

Our question is different: do common mathematical symbols already function as ``instruction primitives'' that models understand from their training? Symbols like $\in$ for membership or $\Rightarrow$ for implication appear throughout mathematical and programming texts. If models have internalized their meaning, we could use them directly without explicit teaching. This idea has precedent in legal drafting, where symbolic logic reduces ambiguity in contracts \citep{allen1957symbolic}. A prompt, like a contract, specifies intent. Structured symbols might express that intent more clearly than prose.

We introduce MetaGlyph, a symbolic instruction language that uses mathematical notation to compress prompts. MetaGlyph is not a learned system or a codebook requiring explanation. It reuses symbols that models have seen extensively during training: membership ($\in$), negation ($\neg$), conjunction ($\cap$), and implication ($\Rightarrow$). The goal is to express the same constraints with fewer, more precise tokens.

We test whether MetaGlyph preserves instruction meaning under controlled conditions. For each task, we compare three prompts: natural language, MetaGlyph, and a control that looks symbolic but uses meaningless symbols. This design separates genuine semantic compression from superficial ``looks mathematical'' effects.

The paper proceeds as follows: Section~\ref{sec:framing} explains why prompts can be viewed as metalanguages. Section~\ref{sec:related} reviews related work. Section~\ref{sec:methodology} defines MetaGlyph's symbols and grammar. Section~\ref{sec:results} reports experimental results. Section~\ref{sec:analysis} analyzes failure modes. Section~\ref{sec:discussion} discusses implications. Section~\ref{sec:conclusion} concludes with limitations and future directions.

\section{Conceptual Framing: Prompts as Specification Languages}
\label{sec:framing}

\subsection{Prompts as Specifications, Not Messages}

We typically think of prompts as messages to a chatbot. But functionally, prompts are closer to \emph{specifications}. They define what should be done with information, not information to be discussed. This makes prompts a kind of metalanguage: a language for describing operations on other content.

Natural language is an inefficient metalanguage. Consider: ``summarize the text, include only technical points, exclude speculation, and format as JSON.'' This sentence encodes four operations (transform, filter, exclude, format) in linear prose, but the operations aren't logically linear; they interact. The model must reconstruct this structure from word cues, while the author must guess how the model will interpret them.

Practitioners have noticed that models often respond more reliably to compact, structured representations than to verbose prose. This suggests models internalize certain ``operator-like'' semantics during training, and effective prompting may mean activating those semantics directly rather than restating them in words.

\subsection{Why Mathematical Symbols Work as Instructions}

Mathematical symbols are attractive for instructions because they're stable, composable, and brief. Models have seen these symbols extensively in training data across math, logic, and programming.

\textbf{Arrows ($\to$, $\Rightarrow$):} In prose, transformation uses varied verbs (``convert,'' ``rewrite,'' ``map''). The arrow $\to$ consistently means ``transforms into.'' The stronger $\Rightarrow$ suggests rules: ``if X then Y.''

\textbf{Membership ($\in$, $\notin$):} Phrases like ``include only'' and ``except for'' are ambiguous about scope. The symbol $x \in S$ unambiguously means ``x belongs to set S.''

\textbf{And/Or ($\cap$, $\cup$):} Natural language ``and'' and ``or'' often obscure whether constraints combine or alternate. The symbol $A \cap B$ clearly means ``both constraints apply''; $A \cup B$ means ``either suffices.''

\textbf{Negation ($\neg$):} Words like ``not,'' ``non-,'' and ``avoid'' differ in scope and emphasis. The symbol $\neg$ has one job: invert the predicate it attaches to.

\subsection{Extended Operators}

Beyond the core operators, additional symbols offer expressive power:

\textbf{Quantifiers ($\forall$, $\exists$):} Words like ``all'' and ``any'' are often ambiguous. The symbol $\forall x \in S$ clearly means ``for every element''; $\exists x \in S$ means ``for at least one.''

\textbf{Subset ($\subseteq$):} While $\in$ specifies element membership, $\subseteq$ specifies that one set is entirely contained in another. This is useful for hierarchical constraints.

\textbf{Mapping ($\mapsto$) and Composition ($\circ$):} The symbol $\mapsto$ signals item-by-item transformation. Composition ($\circ$) encodes sequential operations, replacing awkward phrases like ``first do X, then do Y.''

Not all symbols work equally well. Their usefulness depends on how consistently they appear in training data and whether models treat them as structural markers rather than content. A good symbolic language should be selective, prioritizing stable, well-understood operators.

\subsection{Built-In Versus Taught Semantics}

A key distinction: some symbols carry meaning that models learned during pretraining, while others require explicit teaching. Systems like SynthLang \citep{synthlang2024} work by providing decoding rules in a system prompt, so their success reflects both the symbols and the instructions explaining them. We focus on symbols that work \emph{without} explanation.

This is a strong claim. We're not assuming models ``reason'' formally, but that exposure to consistent symbolic usage has created stable associations. Symbol tuning research shows models can learn arbitrary symbol-behavior mappings, but those are brittle outside their training context. Operators like $\in$ and $\to$, by contrast, carry meaning reinforced across math, logic, and programming.

\subsection{Designing Instruction Languages}

Viewing prompts as specifications reframes prompt engineering as language design. MetaGlyph incorporates symbols with high semantic stability, aiming to compress instructions by offloading structure onto symbols. The goal is not just shorter prompts but preserving meaning with fewer tokens. If this works, semantic compression of instructions becomes a viable design principle.

\begin{table}[htbp]
\centering
\caption{Candidate Symbolic Operators for Instruction Metalanguages}
\label{tab:operators}
\begin{tabular}{@{}llll@{}}
\toprule
Operator & Domain Origin & Canonical Meaning & Instruction-Semantic Role \\
\midrule
$\to$ & Logic / Math & Directional implication & Transformation / mapping \\
$\Rightarrow$ & Logic & Strong implication & Rule-like constraint \\
$\in$ & Set theory & Membership & Inclusion / filtering \\
$\notin$ & Set theory & Non-membership & Exclusion \\
$\cap$ & Set theory & Intersection & Conjunctive constraint \\
$\cup$ & Set theory & Union & Disjunctive constraint \\
$\neg$ & Logic & Negation & Prohibition / exclusion \\
$\forall$ & Logic & Universal quantifier & Apply constraint to all \\
$\exists$ & Logic & Existential quantifier & At least one satisfies \\
$\subseteq$ & Set theory & Subset & Hierarchical constraint \\
$\mapsto$ & Math / CS & Anonymous mapping & Pointwise transformation \\
$\circ$ & Math & Composition & Sequential operations \\
$|$ & Math / CS & Such-that / restriction & Scope delimitation \\
\bottomrule
\end{tabular}
\end{table}

\section{Related Work}
\label{sec:related}

Related work spans several areas, each treating instruction semantics differently.

\paragraph{Prompt Compression.}
Methods like LLMLingua \citep{jiang2023llmlingua,jiang2024llmlingua2} reduce prompt length by pruning or summarizing \emph{content}. These achieve significant token savings but leave the instruction language untouched.

\paragraph{Prompt Distillation.}
Distillation approaches learn shorter prompts from longer ones via gradient tuning or soft prompts. The resulting representations are task-specific artifacts whose semantics come from optimization, not linguistic convention. We instead ask whether models already understand certain symbols from pretraining.

\paragraph{Symbol Tuning.}
Research shows models can learn arbitrary symbol-behavior mappings through training \citep{wei2023symbol}. But these symbols are intentionally meaningless before tuning. We focus on symbols with established meaning across domains.

\paragraph{Constructed Prompt Languages.}
Systems like SynthLang \citep{synthlang2024} work by providing explicit decoding rules. This conflates the symbols' expressivity with the instructions teaching them. We test whether symbols work \emph{without} such instructions.

\paragraph{Structured Prompting and Adversarial Work.}
Pseudo-code and chain-of-symbol prompting show models leverage symbolic structure for reasoning. Adversarial research shows symbolic encodings can bypass safety constraints \citep{bethany2024jailbreaking}. Neither provides a constructive account of instruction semantics.

\paragraph{Our Position.}
We focus on compressing the \emph{instruction language itself} using symbols whose meaning comes from pretraining, not from learned mappings or explicit decoding rules.

\section{Methodology}
\label{sec:methodology}

\subsection{Overview}

We test whether symbolic instructions preserve meaning compared to natural language. For each task, we compare three prompt types:
\begin{enumerate}
    \item \textbf{Natural language (NL):} Verbose prose instructions
    \item \textbf{MetaGlyph (MG):} Symbolic instructions using mathematical operators
    \item \textbf{Control (CTRL):} Symbolic-looking prompts with meaningless symbols
\end{enumerate}

The control condition ensures any benefits come from the \emph{meaning} of the symbols, not just their appearance.

\subsection{MetaGlyph Operators}

MetaGlyph uses a small set of mathematical symbols as instruction primitives (Table~\ref{tab:metaglyph-ops}).

\begin{table}[htbp]
\centering
\caption{MetaGlyph Operator Inventory}
\label{tab:metaglyph-ops}
\begin{tabular}{@{}lll@{}}
\toprule
Operator & Function & Example Usage \\
\midrule
$\in$ & Membership / inclusion & \texttt{$\in$(mammal)} (item belongs to category) \\
$\neg$ & Negation / exclusion & \texttt{$\neg$(bird)} (item must not be in category) \\
$\cap$ & Intersection / conjunction & \texttt{$\in$(pet) $\cap$ $\in$(mammal)} (both constraints apply) \\
$\to$ & Transformation / mapping & \texttt{items $\to$ select} (transform input to output) \\
$\Rightarrow$ & Implication / conditional & \texttt{$\in$(admin) $\Rightarrow$ access=full} (if-then rule) \\
$\circ$ & Composition / sequencing & \texttt{filter $\circ$ sort} (apply operations in order) \\
\bottomrule
\end{tabular}
\end{table}

Instructions follow a simple grammar: an \textbf{input anchor} (\texttt{items}), a \textbf{constraint clause} (\texttt{$\in$(mammal) $\cap$ $\neg$(bird)}), and a \textbf{task clause} (\texttt{$\Rightarrow$ select}). MetaGlyph is hybrid: labels use plain words while structure uses symbols. Crucially, no legends or explanations accompany the symbols.

\subsection{Prompt Variants}

For each test, we create three prompts with identical input but different instruction formats:

\textbf{NL:} Verbose prose (avg.\ 172 tokens). \textbf{MG:} Symbolic (avg.\ 51 tokens). \textbf{CTRL:} Same structure as MG but with meaningless symbols ($\odot$, $\otimes$, $\oplus$) replacing the operators.

All variants request JSON output.

\subsection{Token Control}

We use whitespace-based tokenization for consistent counts. CTRL prompts match MG token counts exactly; the difference between NL and MG represents the compression ratio.

\subsection{Tasks}

We test four task families, each probing different operators (150 instances each):

\textbf{Selection:} Select items matching inclusion/exclusion constraints ($\in$, $\neg$). Example: ``select mammals that are pets but not birds.''

\textbf{Extraction:} Extract specific risk categories from technical reports into JSON format.

\textbf{Constraint composition:} Apply multiple simultaneous constraints using $\cap$ (conjunction).

\textbf{Conditional transformation:} Apply if-then rules ($\Rightarrow$) and sequential operations ($\circ$). Example: ``if employee, label as internal; then lowercase names.''

\subsection{Models}

We evaluate five instruction-tuned models spanning 3B to 1T parameters (Table~\ref{tab:models}). All use deterministic decoding (temperature 0).

\begin{table}[htbp]
\centering
\caption{Evaluated Models}
\label{tab:models}
\begin{tabular}{@{}lll@{}}
\toprule
Model & Parameters & OpenRouter ID \\
\midrule
Llama 3.2 3B Instruct & 3B & \texttt{meta-llama/llama-3.2-3b-instruct} \\
Gemma 3 12B IT & 12B & \texttt{google/gemma-3-12b-it} \\
Qwen 2.5 7B Instruct & 7B & \texttt{qwen/qwen-2.5-7b-instruct} \\
OLMo 3 7B Instruct & 7B & \texttt{allenai/olmo-3-7b-instruct} \\
Kimi K2 & 1T (32B active) & \texttt{moonshotai/kimi-k2} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Metrics}

We measure semantic equivalence (how often MG and NL produce identical outputs, independent of correctness), operator fidelity (whether each operator works as intended, such as $\in$ correctly filtering to the specified category), and compression ratio (token reduction from NL to MG).

\subsection{Reproducibility}

All prompts, outputs, and evaluation code are publicly available. Tasks are benign; we do not evaluate adversarial uses.

\section{Results}
\label{sec:results}

\subsection{Token Compression}

MetaGlyph achieves 62--81\% token reduction across all task families (Table~\ref{tab:compression}).

\begin{table}[htbp]
\centering
\caption{Token Compression by Task Family}
\label{tab:compression}
\begin{tabular}{@{}lcccc@{}}
\toprule
Task Family & NL Tokens & MG Tokens & CTRL Tokens & Reduction \\
\midrule
Selection \& Classification & 215 & 41 & 41 & \textbf{80.9\%} \\
Structured Extraction & 176 & 52 & 52 & \textbf{70.5\%} \\
Constraint Composition & 134 & 48 & 48 & \textbf{64.2\%} \\
Conditional Transformation & 164 & 62 & 62 & \textbf{62.2\%} \\
\bottomrule
\end{tabular}
\end{table}

Selection tasks compress most (80.9\%) because phrases like ``belongs to the category of mammals'' become simply \texttt{$\in$(mammal)}. Conditional tasks compress least (62.2\%) because rules need more specification even symbolically.

\subsection{Semantic Equivalence}

We measure how often MG and NL produce identical outputs, regardless of whether those outputs are correct. This isolates whether the compression preserves meaning (Table~\ref{tab:equivalence}).

\begin{table}[htbp]
\centering
\caption{Output Equivalence (NL $=$ MG) by Model and Task Family}
\label{tab:equivalence}
\begin{tabular}{@{}lccccc@{}}
\toprule
Model & Selection & Extraction & Constraint & Transformation & NL $=$ CTRL \\
\midrule
Llama 3.2 3B & 0\% & 2\% & 0\% & 34\% & \textbf{0\%} \\
Gemma 3 12B & 0\% & 20\% & 0\% & 0\% & \textbf{0\%} \\
Qwen 2.5 7B & 8\% & 44\% & 0.7\% & 0\% & \textbf{0\%} \\
OLMo 3 7B & 0\% & 0\% & 0.7\% & 0\% & \textbf{0\%} \\
Kimi K2 & 8\% & 0\% & 0\% & 0.7\% & \textbf{0\%} \\
\bottomrule
\end{tabular}
\end{table}

The results reveal several patterns. The controls work as intended: NL$=$CTRL is 0\% across all models, confirming that meaningless symbols produce different outputs than meaningful ones. Extraction works best, with Qwen achieving 44\% equivalence on extraction tasks. This means nearly half the time, MG and NL produce identical outputs. Constraint composition fails: near-zero equivalence indicates models do not reliably interpret $\cap$ as ``apply both constraints.'' An important nuance is that equivalence does not imply correctness. MG can preserve NL semantics even when both produce wrong answers. When they are ``wrong together,'' this validates the compression as meaning-preserving even when the underlying task is difficult.

\subsection{Parse Success}

Larger models (Gemma 12B) achieve 100\% JSON parse success. Smaller models (Llama 3B) struggle with symbolic prompts, often producing verbose explanations instead of structured output.

\subsection{Operator Fidelity}

Which symbols work? Table~\ref{tab:fidelity} shows fidelity by operator.

\begin{table}[htbp]
\centering
\caption{Operator Fidelity by Model}
\label{tab:fidelity}
\begin{tabular}{@{}lccccc@{}}
\toprule
Operator & Llama 3.2 3B & Gemma 3 12B & Qwen 2.5 7B & OLMo 3 7B & Kimi K2 \\
\midrule
$\in$ (membership) & 33.3\% & 0.0\% & 18.9\% & 0.0\% & \textbf{36.0\%} \\
$\to$ (transformation) & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% \\
$\cap$ (intersection) & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% \\
$\Rightarrow$ (implication) & 0.0\% & 0.0\% & 0.0\% & 0.0\% & \textbf{98.1\%} \\
\bottomrule
\end{tabular}
\end{table}

The results reveal a scale-dependent pattern. Smaller models (Llama 3B) show moderate $\in$ fidelity (33.3\%), while the largest model tested---Kimi K2 (1T parameters, 32B active)---demonstrates dramatically higher operator fidelity: 36\% for membership ($\in$) and a remarkable 98.1\% for implication ($\Rightarrow$). This suggests that frontier-scale models may have internalized symbolic semantics more robustly than mid-sized models. Notably, mid-range models (Gemma 12B, OLMo 7B) show 0\% fidelity across all operators, suggesting a U-shaped relationship where instruction tuning in mid-sized models creates stronger natural-language biases that larger models can overcome.

\subsection{Response Time}

Preliminary data suggests MG prompts are $\sim$30\% faster than NL due to shorter input. Interestingly, CTRL prompts are slowest; semantic incoherence may increase processing overhead as models struggle to interpret meaningless symbols.

\section{Analysis}
\label{sec:analysis}

\subsection{Why Some Operators Fail}

Models do not reliably parse symbolic operators as constraints. Given \texttt{$\in$(mammal) $\cap$ $\in$(pet) $\cap$ $\neg$(bird)}, the intended reading is ``mammals AND pets AND not birds.'' But models often treat this as a list or ignore the negation.

The problem is that in math texts, $\cap$ connects \emph{sets}, whereas here it connects \emph{predicates}. This is a generalization models have not learned. Similarly, $\Rightarrow$ appears in diverse contexts with varying interpretations: mathematical proofs, programming, logical specifications. Mid-sized models do not reliably treat it as ``if-then,'' though frontier-scale models (Kimi K2) overcome this limitation.

\subsection{Model Scale Effects}

Larger models parse better (Gemma 12B: 100\% JSON success vs.\ Llama 3B: 30\%). But the relationship between scale and operator fidelity is non-monotonic. Mid-sized models (7B--12B) show \emph{worse} operator fidelity than smaller models (Gemma: 0\% vs.\ Llama: 33\% on $\in$), suggesting instruction tuning creates stronger natural-language biases. However, frontier-scale models like Kimi K2 (1T parameters) recover and exceed small-model fidelity, achieving 36\% on $\in$ and 98.1\% on $\Rightarrow$. This U-shaped curve suggests that sufficient scale can overcome instruction-tuning biases and access pretrained symbolic semantics.

\subsection{Control Validation}

Early experiments showed high NL$=$CTRL equivalence, which turned out to be a bug. The control prompts were not replacing \emph{all} operators with nonsense symbols. After fixing this, CTRL equivalence dropped to 0\%, confirming models \emph{do} respond to symbolic meaning, not just symbolic appearance.

\section{Discussion}
\label{sec:discussion}

\subsection{Practical Takeaways}

Several practical lessons emerge from this experiment. Compression works: 62--81\% token reduction is achievable. Model choice matters significantly: frontier-scale models (Kimi K2) show dramatically higher operator fidelity than mid-sized models, with 98.1\% fidelity for implication ($\Rightarrow$). Operators must be chosen carefully: $\in$ (membership) and $\Rightarrow$ (implication) work well at scale, while $\cap$ (intersection) remains unreliable across all models. Hybrid prompts combining symbols for structure with natural language for critical constraints may work best for mid-sized models.

\subsection{What We Showed}

Symbolic compression preserves meaning up to 44\% of the time for extraction tasks. The 0\% CTRL equivalence confirms models respond to symbolic \emph{meaning}, not just appearance. Operator reliability varies by scale: mid-sized models struggle with all operators, while frontier-scale models (Kimi K2) achieve near-perfect fidelity (98.1\%) for implication ($\Rightarrow$) and strong fidelity (36\%) for membership ($\in$).

\subsection{What We Did Not Show}

We have not demonstrated system-level latency improvements (timing is preliminary), generalization to other models, or whether fine-tuning could improve operator fidelity. This was a short experiment designed to test the theory that pretrained symbolic semantics could enable instruction compression; it does not provide a complete characterization of when and why symbolic instruction languages succeed or fail.

\subsection{Limitations and Future Work}

This work establishes a baseline rather than characterizing limits. The experiment was intentionally narrow: four task families, five models spanning 3B to 1T parameters, and a single symbolic language design. Many questions remain open.

Future work should systematically stress-test symbolic instruction languages. Scope binding tests could determine whether $(A \cup B) \cap \neg C$ parses differently from $A \cup (B \cap \neg C)$, revealing whether models correctly associate operators with their intended operands. Implication direction tests could probe whether $\Rightarrow$ is treated as one-way (if-then) or incorrectly as equivalence (if-and-only-if). Composition order tests could verify whether $f \circ g$ applies operations in the correct sequence, which matters for tasks where order affects outcomes.

Beyond stress testing, several research directions merit investigation. First, augmented symbolic languages could combine MetaGlyph operators with minimal natural-language anchors or few-shot examples, potentially improving operator fidelity without sacrificing compression. Second, comparative studies across model families could reveal whether certain architectures or training approaches produce better symbolic understanding. Third, fine-tuning experiments could test whether targeted training on symbolic instruction formats improves operator fidelity, and whether such improvements generalize across tasks. Fourth, frontier model evaluation could determine whether larger models (100B+ parameters) show qualitatively different symbolic comprehension. Fifth, density threshold experiments could establish practical compression limits by gradually increasing symbolic density until performance collapses.

The broader goal is to understand which symbols carry robust pretrained semantics and which require explicit teaching, enabling principled design of symbolic instruction languages that balance compression efficiency with semantic reliability.

\section{Conclusion}
\label{sec:conclusion}

We introduced MetaGlyph, a symbolic instruction language that compresses prompts using mathematical notation. The core question was whether models understand these symbols from pretraining, without explicit teaching.

The answer is nuanced and scale-dependent. First, does compression preserve meaning? Partially. Up to 44\% of the time, symbolic and prose instructions produce identical outputs, and the 0\% CTRL equivalence confirms this reflects genuine semantic understanding rather than superficial pattern matching. Second, which symbols work? It depends on model scale. Membership ($\in$) shows 33--36\% fidelity across model sizes. The surprise finding is implication ($\Rightarrow$): while mid-sized models (7B--12B) show zero fidelity, the frontier-scale Kimi K2 (1T parameters) achieves 98.1\% fidelity---near-perfect interpretation of if-then rules. Conjunction ($\cap$) remains unreliable across all models tested.

The practical implications are scale-dependent. For mid-sized models, symbolic compression achieving 62--81\% fewer tokens is achievable, but practitioners should use hybrid approaches combining symbols for structure with natural language for critical constraints. For frontier-scale models, symbolic instructions may work as reliably as natural language while using far fewer tokens.

The relationship between model scale and symbolic comprehension appears U-shaped: small models show moderate fidelity, instruction-tuned mid-sized models show low fidelity due to natural-language biases, and frontier-scale models recover and exceed small-model performance. This suggests that sufficient scale can access pretrained symbolic semantics that instruction tuning temporarily obscures. As models continue scaling, symbolic instruction compression may become increasingly practical.

\section*{Code and Data Availability}

The complete experimental pipeline, prompt templates, raw model outputs, and evaluation scripts are publicly available at: \url{https://github.com/BEAR-LLM-AI/arXiv-Semantic-Compression-of-LLM-Instructions-via-Symbolic-Metalanguages}

\bibliography{references}

\end{document}
