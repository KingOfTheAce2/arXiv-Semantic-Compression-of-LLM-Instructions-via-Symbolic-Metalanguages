\documentclass[11pt,a4paper]{article}

% Core packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}

% Math and symbols
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}

% Tables and figures
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{graphicx}

% Layout and formatting
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{parskip}

% Code listings
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  xleftmargin=0.5em,
  xrightmargin=0.5em
}

% Hyperlinks (load last)
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}

% Bibliography
\usepackage{natbib}
\bibliographystyle{plainnat}

\title{Semantic Compression of LLM Instructions via Symbolic Metalanguages}
\author{Ernst van Gassen\\
  \textit{Independent Researcher}\\
  \texttt{evg.gassen@gmail.com}
}
\date{}

\begin{document}

\maketitle

\begin{abstract}
We introduce MetaGlyph, a symbolic language for compressing prompts by encoding instructions as mathematical symbols rather than prose. Unlike systems requiring explicit decoding rules, MetaGlyph uses symbols like $\in$ (membership) and $\Rightarrow$ (implication) that models already understand from their training data. We test whether these symbols work as ``instruction shortcuts'' that models can interpret without additional teaching.

We evaluate eight models across two dimensions relevant to practitioners: scale (3B--1T parameters) and accessibility (open-source for local deployment vs.\ proprietary APIs). MetaGlyph achieves 62--81\% token reduction across all task types. For API-based deployments, this translates directly to cost savings; for local deployments, it reduces latency and memory pressure.

Results vary dramatically by model. Gemini 2.5 Flash achieves 75\% semantic equivalence between symbolic and prose instructions on selection tasks, with 49.9\% membership operator fidelity. Kimi K2 reaches 98.1\% fidelity for implication ($\Rightarrow$). However, Claude Haiku 4.5 and GPT-5.2 Chat produced no parseable outputs, highlighting format sensitivity in some proprietary models. Mid-sized open-source models (7B--12B) show near-zero operator fidelity, suggesting a U-shaped relationship where sufficient scale overcomes instruction-tuning biases.
\end{abstract}

\medskip
\noindent\textbf{Keywords:} prompt compression, symbolic metalanguages, instruction semantics, large language models, prompt engineering

\section{Introduction}

Large language models are controlled through natural-language prompts. These prose instructions specify tasks and constraints, and the approach works well enough to be widely adopted. But it has two weaknesses. First, prompts are verbose: expressing precise constraints takes many words. Second, prompt behavior is fragile: small wording changes can produce very different outputs.

These problems have real costs. More tokens mean higher inference costs and latency. Ambiguous phrasing leads to inconsistent results. Yet models are trained on vast amounts of mathematical and logical notation, where compact symbols carry precise meaning. This raises a natural question: can we communicate instructions more directly using those symbols?

Most prompt compression research focuses on shortening the \emph{content} being processed: summarizing documents, pruning passages, dropping ``unimportant'' words. But in many prompts, the \emph{instructions themselves} are a substantial fraction of the total length, especially when specifying scope, exclusions, and output format.

Natural language is inherently imprecise for specifying constraints. Words like ``only,'' ``except,'' and ``roughly'' act like logical operators, but their scope is often unclear when multiple constraints interact. If we could express instructions more compactly using symbols, we might gain both efficiency and reliability.

The key question is: can instruction language be compressed while preserving meaning, \emph{without} teaching the model a new symbolic system?

Existing symbolic prompt languages like SynthLang \citep{synthlang2024} show that models can follow symbolic commands, but only when given explicit decoding rules in a system prompt. This conflates two factors: the symbols themselves, and the instructions teaching the model what they mean.

Other research shows models can learn arbitrary symbol-to-behavior mappings through training \citep{wei2023symbol}. Prompt compression systems like LLMLingua \citep{jiang2023llmlingua,jiang2024llmlingua2} shorten content, not instructions. Adversarial research demonstrates that symbolic encodings can bypass safety mechanisms \citep{bethany2024jailbreaking}, confirming that compact symbols affect model behavior.

Our question is different: do common mathematical symbols already function as ``instruction primitives'' that models understand from their training? Symbols like $\in$ for membership or $\Rightarrow$ for implication appear throughout mathematical and programming texts. If models have internalized their meaning, we could use them directly without explicit teaching. This idea has precedent in legal drafting, where symbolic logic reduces ambiguity in contracts \citep{allen1957symbolic}. A prompt, like a contract, specifies intent. Structured symbols might express that intent more clearly than prose.

We introduce MetaGlyph, a symbolic instruction language that uses mathematical notation to compress prompts. MetaGlyph is not a learned system or a codebook requiring explanation. It reuses symbols that models have seen extensively during training: membership ($\in$), negation ($\neg$), conjunction ($\cap$), and implication ($\Rightarrow$). The goal is to express the same constraints with fewer, more precise tokens.

We test whether MetaGlyph preserves instruction meaning under controlled conditions. For each task, we compare three prompts: natural language, MetaGlyph, and a control that looks symbolic but uses meaningless symbols. This design separates genuine semantic compression from superficial ``looks mathematical'' effects.

The paper proceeds as follows: Section~\ref{sec:framing} explains why prompts can be viewed as metalanguages. Section~\ref{sec:related} reviews related work. Section~\ref{sec:methodology} defines MetaGlyph's symbols and grammar. Section~\ref{sec:results} reports experimental results. Section~\ref{sec:analysis} analyzes failure modes. Section~\ref{sec:discussion} discusses implications. Section~\ref{sec:conclusion} concludes with limitations and future directions.

\section{Conceptual Framing: Prompts as Specification Languages}
\label{sec:framing}

\subsection{Prompts as Specifications, Not Messages}

We typically think of prompts as messages to a chatbot. But functionally, prompts are closer to \emph{specifications}. They define what should be done with information, not information to be discussed. This makes prompts a kind of metalanguage: a language for describing operations on other content.

Natural language is an inefficient metalanguage. Consider: ``summarize the text, include only technical points, exclude speculation, and format as JSON.'' This sentence encodes four operations (transform, filter, exclude, format) in linear prose, but the operations aren't logically linear; they interact. The model must reconstruct this structure from word cues, while the author must guess how the model will interpret them.

Practitioners have noticed that models often respond more reliably to compact, structured representations than to verbose prose. This suggests models internalize certain ``operator-like'' semantics during training, and effective prompting may mean activating those semantics directly rather than restating them in words.

\subsection{Why Mathematical Symbols Work as Instructions}

Mathematical symbols are attractive for instructions because they're stable, composable, and brief. Models have seen these symbols extensively in training data across math, logic, and programming.

\textbf{Arrows ($\to$, $\Rightarrow$):} In prose, transformation uses varied verbs (``convert,'' ``rewrite,'' ``map''). The arrow $\to$ consistently means ``transforms into.'' The stronger $\Rightarrow$ suggests rules: ``if X then Y.''

\textbf{Membership ($\in$, $\notin$):} Phrases like ``include only'' and ``except for'' are ambiguous about scope. The symbol $x \in S$ unambiguously means ``x belongs to set S.''

\textbf{And/Or ($\cap$, $\cup$):} Natural language ``and'' and ``or'' often obscure whether constraints combine or alternate. The symbol $A \cap B$ clearly means ``both constraints apply''; $A \cup B$ means ``either suffices.''

\textbf{Negation ($\neg$):} Words like ``not,'' ``non-,'' and ``avoid'' differ in scope and emphasis. The symbol $\neg$ has one job: invert the predicate it attaches to.

\subsection{Extended Operators}

Beyond the core operators, additional symbols offer expressive power:

\textbf{Quantifiers ($\forall$, $\exists$):} Words like ``all'' and ``any'' are often ambiguous. The symbol $\forall x \in S$ clearly means ``for every element''; $\exists x \in S$ means ``for at least one.''

\textbf{Subset ($\subseteq$):} While $\in$ specifies element membership, $\subseteq$ specifies that one set is entirely contained in another. This is useful for hierarchical constraints.

\textbf{Mapping ($\mapsto$) and Composition ($\circ$):} The symbol $\mapsto$ signals item-by-item transformation. Composition ($\circ$) encodes sequential operations, replacing awkward phrases like ``first do X, then do Y.''

Not all symbols work equally well. Their usefulness depends on how consistently they appear in training data and whether models treat them as structural markers rather than content. A good symbolic language should be selective, prioritizing stable, well-understood operators.

\subsection{Built-In Versus Taught Semantics}

A key distinction: some symbols carry meaning that models learned during pretraining, while others require explicit teaching. Systems like SynthLang \citep{synthlang2024} work by providing decoding rules in a system prompt, so their success reflects both the symbols and the instructions explaining them. We focus on symbols that work \emph{without} explanation.

This is a strong claim. We're not assuming models ``reason'' formally, but that exposure to consistent symbolic usage has created stable associations. Symbol tuning research shows models can learn arbitrary symbol-behavior mappings, but those are brittle outside their training context. Operators like $\in$ and $\to$, by contrast, carry meaning reinforced across math, logic, and programming.

\subsection{Designing Instruction Languages}

Viewing prompts as specifications reframes prompt engineering as language design. MetaGlyph incorporates symbols with high semantic stability, aiming to compress instructions by offloading structure onto symbols. The goal is not just shorter prompts but preserving meaning with fewer tokens. If this works, semantic compression of instructions becomes a viable design principle.

\begin{table}[htbp]
\centering
\caption{Candidate Symbolic Operators for Instruction Metalanguages}
\label{tab:operators}
\begin{tabular}{@{}llll@{}}
\toprule
Operator & Domain Origin & Canonical Meaning & Instruction-Semantic Role \\
\midrule
$\to$ & Logic / Math & Directional implication & Transformation / mapping \\
$\Rightarrow$ & Logic & Strong implication & Rule-like constraint \\
$\in$ & Set theory & Membership & Inclusion / filtering \\
$\notin$ & Set theory & Non-membership & Exclusion \\
$\cap$ & Set theory & Intersection & Conjunctive constraint \\
$\cup$ & Set theory & Union & Disjunctive constraint \\
$\neg$ & Logic & Negation & Prohibition / exclusion \\
$\forall$ & Logic & Universal quantifier & Apply constraint to all \\
$\exists$ & Logic & Existential quantifier & At least one satisfies \\
$\subseteq$ & Set theory & Subset & Hierarchical constraint \\
$\mapsto$ & Math / CS & Anonymous mapping & Pointwise transformation \\
$\circ$ & Math & Composition & Sequential operations \\
$|$ & Math / CS & Such-that / restriction & Scope delimitation \\
\bottomrule
\end{tabular}
\end{table}

\section{Related Work}
\label{sec:related}

Related work spans several areas, each treating instruction semantics differently.

\paragraph{Prompt Compression.}
Methods like LLMLingua \citep{jiang2023llmlingua,jiang2024llmlingua2} reduce prompt length by pruning or summarizing \emph{content}. These achieve significant token savings but leave the instruction language untouched.

\paragraph{Prompt Distillation.}
Distillation approaches learn shorter prompts from longer ones via gradient tuning or soft prompts. The resulting representations are task-specific artifacts whose semantics come from optimization, not linguistic convention. We instead ask whether models already understand certain symbols from pretraining.

\paragraph{Symbol Tuning.}
Research shows models can learn arbitrary symbol-behavior mappings through training \citep{wei2023symbol}. But these symbols are intentionally meaningless before tuning. We focus on symbols with established meaning across domains.

\paragraph{Constructed Prompt Languages.}
Systems like SynthLang \citep{synthlang2024} work by providing explicit decoding rules. This conflates the symbols' expressivity with the instructions teaching them. We test whether symbols work \emph{without} such instructions.

\paragraph{Structured Prompting and Adversarial Work.}
Pseudo-code and chain-of-symbol prompting show models leverage symbolic structure for reasoning. Adversarial research shows symbolic encodings can bypass safety constraints \citep{bethany2024jailbreaking}. Neither provides a constructive account of instruction semantics.

\paragraph{Our Position.}
We focus on compressing the \emph{instruction language itself} using symbols whose meaning comes from pretraining, not from learned mappings or explicit decoding rules.

\section{Methodology}
\label{sec:methodology}

\subsection{Overview}

We test whether symbolic instructions preserve meaning compared to natural language. For each task, we compare three prompt types:
\begin{enumerate}
    \item \textbf{Natural language (NL):} Verbose prose instructions
    \item \textbf{MetaGlyph (MG):} Symbolic instructions using mathematical operators
    \item \textbf{Control (CTRL):} Symbolic-looking prompts with meaningless symbols
\end{enumerate}

The control condition ensures any benefits come from the \emph{meaning} of the symbols, not just their appearance.

\subsection{MetaGlyph Operators}

MetaGlyph uses a small set of mathematical symbols as instruction primitives (Table~\ref{tab:metaglyph-ops}).

\begin{table}[htbp]
\centering
\caption{MetaGlyph Operator Inventory}
\label{tab:metaglyph-ops}
\begin{tabular}{@{}lll@{}}
\toprule
Operator & Function & Example Usage \\
\midrule
$\in$ & Membership / inclusion & \texttt{$\in$(mammal)} (item belongs to category) \\
$\neg$ & Negation / exclusion & \texttt{$\neg$(bird)} (item must not be in category) \\
$\cap$ & Intersection / conjunction & \texttt{$\in$(pet) $\cap$ $\in$(mammal)} (both constraints apply) \\
$\to$ & Transformation / mapping & \texttt{items $\to$ select} (transform input to output) \\
$\Rightarrow$ & Implication / conditional & \texttt{$\in$(admin) $\Rightarrow$ access=full} (if-then rule) \\
$\circ$ & Composition / sequencing & \texttt{filter $\circ$ sort} (apply operations in order) \\
\bottomrule
\end{tabular}
\end{table}

Instructions follow a simple grammar: an \textbf{input anchor} (\texttt{items}), a \textbf{constraint clause} (\texttt{$\in$(mammal) $\cap$ $\neg$(bird)}), and a \textbf{task clause} (\texttt{$\Rightarrow$ select}). MetaGlyph is hybrid: labels use plain words while structure uses symbols. Crucially, no legends or explanations accompany the symbols.

\subsection{Prompt Variants}

For each test, we create three prompts with identical input but different instruction formats:

\textbf{NL:} Verbose prose (avg.\ 172 tokens). \textbf{MG:} Symbolic (avg.\ 51 tokens). \textbf{CTRL:} Same structure as MG but with meaningless symbols ($\odot$, $\otimes$, $\oplus$) replacing the operators.

All variants request JSON output.

\subsection{Token Control}

We use whitespace-based tokenization for consistent counts. CTRL prompts match MG token counts exactly; the difference between NL and MG represents the compression ratio.

\subsection{Tasks}

We test four task families, each probing different operators (150 instances each):

\textbf{Selection:} Select items matching inclusion/exclusion constraints ($\in$, $\neg$). Example: ``select mammals that are pets but not birds.''

\textbf{Extraction:} Extract specific risk categories from technical reports into JSON format.

\textbf{Constraint composition:} Apply multiple simultaneous constraints using $\cap$ (conjunction).

\textbf{Conditional transformation:} Apply if-then rules ($\Rightarrow$) and sequential operations ($\circ$). Example: ``if employee, label as internal; then lowercase names.''

\subsection{Models}

We evaluate eight instruction-tuned models along two dimensions relevant to practitioners: \emph{scale} (3B--1T parameters) and \emph{accessibility} (open-source vs.\ proprietary). This design addresses distinct deployment scenarios: researchers and privacy-conscious users who run models locally, and practitioners who access frontier capabilities via commercial APIs (Table~\ref{tab:models}). All experiments use deterministic decoding (temperature 0).

\begin{table}[htbp]
\centering
\caption{Evaluated Models by Scale and Accessibility}
\label{tab:models}
\begin{tabular}{@{}lllll@{}}
\toprule
Model & Parameters & Access & Offline & OpenRouter ID \\
\midrule
Llama 3.2 3B Instruct & 3B & Open & \checkmark & \texttt{meta-llama/llama-3.2-3b-instruct} \\
Qwen 2.5 7B Instruct & 7B & Open & \checkmark & \texttt{qwen/qwen-2.5-7b-instruct} \\
OLMo 3 7B Instruct & 7B & Open & \checkmark & \texttt{allenai/olmo-3-7b-instruct} \\
Gemma 3 12B IT & 12B & Open & \checkmark & \texttt{google/gemma-3-12b-it} \\
\midrule
Kimi K2 & 1T (32B active) & Open & --- & \texttt{moonshotai/kimi-k2} \\
Gemini 2.5 Flash & --- & Proprietary & --- & \texttt{google/gemini-2.5-flash} \\
Claude Haiku 4.5 & --- & Proprietary & --- & \texttt{anthropic/claude-haiku-4.5} \\
GPT-5.2 Chat & --- & Proprietary & --- & \texttt{openai/gpt-5.2-chat} \\
\bottomrule
\end{tabular}
\end{table}

The open-source models (Llama, Qwen, OLMo, Gemma) can be downloaded and run locally without internet connectivity or API costs, making them suitable for privacy-sensitive applications, air-gapped environments, and cost-constrained deployments. The proprietary models (Gemini, Claude, GPT) represent the frontier capabilities that most commercial applications rely on, but require API access and incur per-token costs---precisely where prompt compression offers direct economic benefit.

\subsection{Metrics}

We measure semantic equivalence (how often MG and NL produce identical outputs, independent of correctness), operator fidelity (whether each operator works as intended, such as $\in$ correctly filtering to the specified category), and compression ratio (token reduction from NL to MG).

\subsection{Reproducibility}

All prompts, outputs, and evaluation code are publicly available. Tasks are benign; we do not evaluate adversarial uses.

\section{Results}
\label{sec:results}

\subsection{Token Compression}

MetaGlyph achieves 62--81\% token reduction across all task families (Table~\ref{tab:compression}).

\begin{table}[htbp]
\centering
\caption{Token Compression by Task Family}
\label{tab:compression}
\begin{tabular}{@{}lcccc@{}}
\toprule
Task Family & NL Tokens & MG Tokens & CTRL Tokens & Reduction \\
\midrule
Selection \& Classification & 215 & 41 & 41 & \textbf{80.9\%} \\
Structured Extraction & 176 & 52 & 52 & \textbf{70.5\%} \\
Constraint Composition & 134 & 48 & 48 & \textbf{64.2\%} \\
Conditional Transformation & 164 & 62 & 62 & \textbf{62.2\%} \\
\bottomrule
\end{tabular}
\end{table}

Selection tasks compress most (80.9\%) because phrases like ``belongs to the category of mammals'' become simply \texttt{$\in$(mammal)}. Conditional tasks compress least (62.2\%) because rules need more specification even symbolically.

\subsection{Semantic Equivalence}

We measure how often MG and NL produce identical outputs, regardless of whether those outputs are correct. This isolates whether the compression preserves meaning (Table~\ref{tab:equivalence}).

\begin{table}[htbp]
\centering
\caption{Output Equivalence (NL $=$ MG) by Model and Task Family}
\label{tab:equivalence}
\begin{tabular}{@{}llccccc@{}}
\toprule
Model & Access & Selection & Extraction & Constraint & Transform. & NL $=$ CTRL \\
\midrule
Llama 3.2 3B & Open & 0\% & 2\% & 0\% & 34\% & \textbf{0\%} \\
Qwen 2.5 7B & Open & 8\% & 44\% & 0.7\% & 0\% & \textbf{0\%} \\
OLMo 3 7B & Open & 0\% & 0\% & 0.7\% & 0\% & \textbf{0\%} \\
Gemma 3 12B & Open & 0\% & 20\% & 0\% & 0\% & \textbf{0\%} \\
\midrule
Kimi K2 & Open & 8\% & 0\% & 0\% & 0.7\% & \textbf{0\%} \\
Gemini 2.5 Flash & Prop. & \textbf{75\%} & 12.5\% & 4\% & 0\% & \textbf{0\%} \\
Claude Haiku 4.5 & Prop. & \multicolumn{4}{c}{--- (0\% parse success)\textsuperscript{*}} & --- \\
GPT-5.2 Chat & Prop. & \multicolumn{4}{c}{--- (0\% parse success)\textsuperscript{*}} & --- \\
\bottomrule
\multicolumn{7}{l}{\textsuperscript{*}\footnotesize Claude and GPT produced non-JSON outputs; see Section~\ref{sec:analysis}.}
\end{tabular}
\end{table}

The results reveal several patterns. The controls work as intended: NL$=$CTRL is 0\% across all working models, confirming that meaningless symbols produce different outputs than meaningful ones. Gemini 2.5 Flash achieves remarkable 75\% equivalence on selection tasks---the highest observed---meaning symbolic and prose instructions produce identical outputs three-quarters of the time. Among open-source models, Qwen achieves 44\% equivalence on extraction tasks. Constraint composition fails across all models: near-zero equivalence indicates models do not reliably interpret $\cap$ as ``apply both constraints.''

Two proprietary models (Claude Haiku 4.5, GPT-5.2 Chat) produced 0\% parseable JSON outputs across all conditions. This failure mode, discussed in Section~\ref{sec:analysis}, highlights format sensitivity in frontier models.

\subsection{Parse Success}

Larger models (Gemma 12B) achieve 100\% JSON parse success. Smaller models (Llama 3B) struggle with symbolic prompts, often producing verbose explanations instead of structured output.

\subsection{Operator Fidelity}

Which symbols work? Table~\ref{tab:fidelity} shows fidelity by operator across models with valid outputs.

\begin{table}[htbp]
\centering
\caption{Operator Fidelity by Model (Open-Source and Proprietary)}
\label{tab:fidelity}
\begin{tabular}{@{}lcccccc@{}}
\toprule
Operator & Llama 3B & Qwen 7B & OLMo 7B & Gemma 12B & Kimi K2 & Gemini 2.5 \\
\midrule
$\in$ (membership) & 33.3\% & 18.9\% & 0.0\% & 0.0\% & 36.0\% & \textbf{49.9\%} \\
$\to$ (transformation) & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% \\
$\cap$ (intersection) & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 2.7\% \\
$\Rightarrow$ (implication) & 0.0\% & 0.0\% & 0.0\% & 0.0\% & \textbf{98.1\%} & 33.5\% \\
\bottomrule
\multicolumn{7}{l}{\footnotesize Claude Haiku 4.5 and GPT-5.2 Chat excluded due to 0\% parse success.}
\end{tabular}
\end{table}

The results reveal patterns across both model families. Among open-source models, Llama 3B shows moderate $\in$ fidelity (33.3\%), while mid-range models (Gemma 12B, OLMo 7B) show 0\% fidelity---a U-shaped pattern suggesting instruction tuning creates natural-language biases.

Among proprietary models with valid outputs, Gemini 2.5 Flash achieves the highest membership fidelity (\textbf{49.9\%}) and moderate implication fidelity (33.5\%). Kimi K2 shows remarkable implication fidelity (\textbf{98.1\%}) but only 36\% on membership. This suggests different models have internalized different operators more strongly, possibly reflecting training data composition.

\subsection{Response Time}

Preliminary data suggests MG prompts are $\sim$30\% faster than NL due to shorter input. Interestingly, CTRL prompts are slowest; semantic incoherence may increase processing overhead as models struggle to interpret meaningless symbols.

\section{Analysis}
\label{sec:analysis}

\subsection{Why Some Operators Fail}

Models do not reliably parse symbolic operators as constraints. Given \texttt{$\in$(mammal) $\cap$ $\in$(pet) $\cap$ $\neg$(bird)}, the intended reading is ``mammals AND pets AND not birds.'' But models often treat this as a list or ignore the negation.

The problem is that in math texts, $\cap$ connects \emph{sets}, whereas here it connects \emph{predicates}. This is a generalization models have not learned. Similarly, $\Rightarrow$ appears in diverse contexts with varying interpretations: mathematical proofs, programming, logical specifications. Mid-sized models do not reliably treat it as ``if-then,'' though frontier-scale models (Kimi K2) overcome this limitation.

\subsection{Model Scale Effects}

Larger models parse better (Gemma 12B: 100\% JSON success vs.\ Llama 3B: 30\%). But the relationship between scale and operator fidelity is non-monotonic. Mid-sized open-source models (7B--12B) show \emph{worse} operator fidelity than smaller models (Gemma: 0\% vs.\ Llama: 33\% on $\in$), suggesting instruction tuning creates stronger natural-language biases. However, frontier-scale models recover and exceed small-model fidelity: Kimi K2 achieves 98.1\% on $\Rightarrow$, while Gemini 2.5 Flash achieves 49.9\% on $\in$. This U-shaped curve suggests that sufficient scale can overcome instruction-tuning biases and access pretrained symbolic semantics.

\subsection{Proprietary Model Format Sensitivity}

Two proprietary models---Claude Haiku 4.5 and GPT-5.2 Chat---produced 0\% parseable JSON outputs across all conditions (NL, MG, and CTRL). This complete failure occurred despite identical prompts working with other models. Possible explanations include:

\begin{itemize}
    \item \textbf{Aggressive safety filtering}: These models may interpret symbolic notation or certain task phrasings as potentially harmful and refuse to produce structured outputs.
    \item \textbf{Format preference shifts}: Recent model updates may have changed default output formatting, preferring markdown or conversational responses over raw JSON.
    \item \textbf{API configuration}: Default parameters may differ from other providers, requiring explicit JSON mode flags not used in our unified pipeline.
\end{itemize}

This finding has practical implications: symbolic compression research must account for model-specific output behaviors, and practitioners should verify format compliance before deploying compressed prompts.

\subsection{Control Validation}

Early experiments showed high NL$=$CTRL equivalence, which turned out to be a bug. The control prompts were not replacing \emph{all} operators with nonsense symbols. After fixing this, CTRL equivalence dropped to 0\%, confirming models \emph{do} respond to symbolic meaning, not just symbolic appearance.

\section{Discussion}
\label{sec:discussion}

\subsection{Practical Takeaways}

Several practical lessons emerge, differentiated by deployment scenario:

\textbf{For Gemini deployments}: Symbolic compression is highly effective. Gemini 2.5 Flash achieves 75\% semantic equivalence and 49.9\% membership fidelity---the best results observed. With 62--81\% token reduction, this translates to significant cost savings for high-volume applications.

\textbf{For Kimi K2 deployments}: Implication-heavy prompts work exceptionally well (98.1\% fidelity). Use $\Rightarrow$ for conditional logic and if-then rules.

\textbf{For local/offline deployments} (Llama, Qwen, OLMo, Gemma): Token costs are zero, but compression still reduces memory pressure and latency. These smaller models show lower operator fidelity, requiring hybrid approaches that combine symbols for structure with natural language for critical constraints.

\textbf{For Claude and GPT}: Exercise caution. Our experiments produced 0\% parseable outputs, suggesting these models may require explicit JSON mode configuration or different prompt formatting. Verify format compliance before deployment.

\textbf{Operator selection is critical}: $\in$ (membership) works best across models (up to 49.9\%), $\Rightarrow$ (implication) shows high variance (0--98\%), and $\cap$ (intersection) remains unreliable everywhere. Always test on your target model.

\subsection{What We Showed}

Symbolic compression preserves meaning up to 75\% of the time for selection tasks (Gemini 2.5 Flash) and 44\% for extraction tasks (Qwen 2.5). The 0\% CTRL equivalence confirms models respond to symbolic \emph{meaning}, not just appearance. Operator reliability varies by model: Gemini 2.5 Flash achieves the highest membership fidelity (49.9\%), while Kimi K2 achieves near-perfect implication fidelity (98.1\%). However, two proprietary models (Claude, GPT) failed to produce parseable outputs entirely, demonstrating that symbolic prompts require format verification per model.

\subsection{What We Did Not Show}

We have not demonstrated system-level latency improvements (timing is preliminary), generalization to other models, or whether fine-tuning could improve operator fidelity. This was a short experiment designed to test the theory that pretrained symbolic semantics could enable instruction compression; it does not provide a complete characterization of when and why symbolic instruction languages succeed or fail.

\subsection{Limitations and Future Work}

This work establishes a baseline rather than characterizing limits. The experiment was intentionally narrow: four task families, five models spanning 3B to 1T parameters, and a single symbolic language design. Many questions remain open.

Future work should systematically stress-test symbolic instruction languages. Scope binding tests could determine whether $(A \cup B) \cap \neg C$ parses differently from $A \cup (B \cap \neg C)$, revealing whether models correctly associate operators with their intended operands. Implication direction tests could probe whether $\Rightarrow$ is treated as one-way (if-then) or incorrectly as equivalence (if-and-only-if). Composition order tests could verify whether $f \circ g$ applies operations in the correct sequence, which matters for tasks where order affects outcomes.

Beyond stress testing, several research directions merit investigation. First, augmented symbolic languages could combine MetaGlyph operators with minimal natural-language anchors or few-shot examples, potentially improving operator fidelity without sacrificing compression. Second, comparative studies across model families could reveal whether certain architectures or training approaches produce better symbolic understanding. Third, fine-tuning experiments could test whether targeted training on symbolic instruction formats improves operator fidelity, and whether such improvements generalize across tasks. Fourth, frontier model evaluation could determine whether larger models (100B+ parameters) show qualitatively different symbolic comprehension. Fifth, density threshold experiments could establish practical compression limits by gradually increasing symbolic density until performance collapses.

The broader goal is to understand which symbols carry robust pretrained semantics and which require explicit teaching, enabling principled design of symbolic instruction languages that balance compression efficiency with semantic reliability.

\section{Conclusion}
\label{sec:conclusion}

We introduced MetaGlyph, a symbolic instruction language that compresses prompts using mathematical notation. The core question was whether models understand these symbols from pretraining, without explicit teaching.

The answer is nuanced and model-dependent. First, does compression preserve meaning? Yes, substantially for certain models. Gemini 2.5 Flash achieves 75\% semantic equivalence on selection tasks---symbolic and prose instructions produce identical outputs three-quarters of the time. The 0\% CTRL equivalence confirms this reflects genuine semantic understanding rather than superficial pattern matching.

Second, which symbols work? It depends on the model. Membership ($\in$) works best overall, with Gemini 2.5 Flash achieving 49.9\% fidelity. Implication ($\Rightarrow$) shows the widest variance: 98.1\% fidelity on Kimi K2, 33.5\% on Gemini, and 0\% on mid-sized models. Conjunction ($\cap$) remains unreliable across all models tested.

Third, can all models use symbolic compression? No. Claude Haiku 4.5 and GPT-5.2 Chat produced zero parseable outputs across all conditions, highlighting that format sensitivity varies significantly across proprietary APIs.

The practical implications: for Gemini-based deployments, symbolic compression is highly effective with 75\% meaning preservation and 50\% operator fidelity. For Kimi K2, implication-heavy prompts compress reliably. For mid-sized open-source models, hybrid approaches combining symbols with natural language work best. For Claude and GPT, additional format configuration may be required.

The relationship between model scale and symbolic comprehension appears U-shaped among working models: small models show moderate fidelity, instruction-tuned mid-sized models show low fidelity, and frontier-scale models recover and exceed small-model performance. For practitioners: always verify format compliance and operator fidelity on your target model before deploying compressed prompts in production.

\section*{Code and Data Availability}

The complete experimental pipeline, prompt templates, raw model outputs, and evaluation scripts are publicly available at: \url{https://github.com/BEAR-LLM-AI/arXiv-Semantic-Compression-of-LLM-Instructions-via-Symbolic-Metalanguages}

\bibliography{references}

\end{document}
